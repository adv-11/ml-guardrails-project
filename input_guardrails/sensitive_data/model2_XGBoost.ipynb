{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cxWAgexg-GuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ebe95a-544b-4212-e02e-3069f4d15ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df = pd.read_pickle(\"/content/drive/My Drive/CMPE 257/CMPE 257 Colab/257 Sensitive Data Input Guardrail/train_df_embedding.pkl\")\n",
        "test_df = pd.read_pickle(\"/content/drive/My Drive/CMPE 257/CMPE 257 Colab/257 Sensitive Data Input Guardrail/test_df_embedding.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# prepare the data\n",
        "X_train_texts = train_df[\"source_text\"]\n",
        "y_train = train_df[\"is_sensitive\"]\n",
        "\n",
        "X_test_texts = test_df[\"source_text\"]\n",
        "y_test = test_df[\"is_sensitive\"]\n",
        "\n",
        "# vectorize text with TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features = 1500, ngram_range = (1, 2), stop_words = \"english\")\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train_texts)\n",
        "X_test = vectorizer.transform(X_test_texts)\n",
        "\n",
        "# train xgboost classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective = \"binary:logistic\", eval_metric = \"logloss\", max_depth = None, learning_rate = 0.1, n_estimators = 500,\n",
        "    subsample = 0.8, # random rows to prevent overfitting and add diversity\n",
        "    colsample_bytree = 0.8, # random columns to reduces feature correlation\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# evaluate\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits = 3))\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = xgb_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "\n",
        "print(\"\\nTop 20 Important Words for Sensitive Detection:\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38IdgkusBu7j",
        "outputId": "665cfb31-e38a-4742-d7f6-f80b401c76d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9209665240372514\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.708     0.525     0.603       908\n",
            "           1      0.941     0.972     0.956      7038\n",
            "\n",
            "    accuracy                          0.921      7946\n",
            "   macro avg      0.824     0.749     0.780      7946\n",
            "weighted avg      0.914     0.921     0.916      7946\n",
            "\n",
            "\n",
            "Top 20 Important Words for Sensitive Detection:\n",
            "com: 0.0215\n",
            "street: 0.0156\n",
            "passport: 0.0112\n",
            "number: 0.0111\n",
            "road: 0.0103\n",
            "eng: 0.0094\n",
            "username: 0.0090\n",
            "ip: 0.0075\n",
            "sex: 0.0067\n",
            "11: 0.0062\n",
            "city: 0.0059\n",
            "id: 0.0058\n",
            "pm: 0.0055\n",
            "postcode: 0.0054\n",
            "country: 0.0054\n",
            "clock: 0.0052\n",
            "head meta: 0.0051\n",
            "introduction: 0.0049\n",
            "02: 0.0048\n",
            "password: 0.0047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings and PCA with 80% variance\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# building feature matrices from embedding column\n",
        "X_train = np.vstack(train_df[\"embedding\"].values)  # shape - (n_train, embedding_dim)\n",
        "X_test  = np.vstack(test_df[\"embedding\"].values)   # shape - (n_test, embedding_dim)\n",
        "\n",
        "y_train = train_df[\"is_sensitive\"].values\n",
        "y_test  = test_df[\"is_sensitive\"].values\n",
        "\n",
        "\n",
        "# scaling embeddings before logistic regression\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# fit pca with 114 components\n",
        "pca = PCA(n_components = 114) # 114 components to capture 80% of variance\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# train xgboost classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective = \"binary:logistic\", eval_metric = \"logloss\", max_depth = None, learning_rate = 0.15, n_estimators = 1100,\n",
        "    subsample = 0.8, # random rows to prevent overfitting and add diversity\n",
        "    colsample_bytree = 0.8, # random columns to reduces feature correlation\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# evaluate\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits = 3))\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = xgb_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "\n",
        "print(\"\\nTop 20 Important Words for Sensitive Detection:\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF_3Sgk0OEv-",
        "outputId": "a5ae39fa-85b2-4528-c7f6-698a1c1f0bf5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9288950415303298\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.754     0.561     0.643       908\n",
            "           1      0.945     0.976     0.961      7038\n",
            "\n",
            "    accuracy                          0.929      7946\n",
            "   macro avg      0.850     0.768     0.802      7946\n",
            "weighted avg      0.923     0.929     0.924      7946\n",
            "\n",
            "\n",
            "Top 20 Important Words for Sensitive Detection:\n",
            "00: 0.1039\n",
            "00 00: 0.0390\n",
            "000: 0.0278\n",
            "12th: 0.0254\n",
            "03: 0.0248\n",
            "10th: 0.0190\n",
            "06: 0.0171\n",
            "15th: 0.0160\n",
            "17: 0.0149\n",
            "1988: 0.0127\n",
            "17th: 0.0124\n",
            "1987: 0.0123\n",
            "08: 0.0118\n",
            "11th: 0.0109\n",
            "150: 0.0109\n",
            "1995: 0.0109\n",
            "00 email: 0.0104\n",
            "14th: 0.0101\n",
            "1980: 0.0099\n",
            "1975: 0.0098\n"
          ]
        }
      ]
    }
  ]
}