{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df = pd.read_pickle(\"/content/drive/My Drive/CMPE 257/CMPE 257 Colab/257 Sensitive Data Input Guardrail/train_df_embedding.pkl\")\n",
        "test_df = pd.read_pickle(\"/content/drive/My Drive/CMPE 257/CMPE 257 Colab/257 Sensitive Data Input Guardrail/test_df_embedding.pkl\")\n"
      ],
      "metadata": {
        "id": "aijNo4R1S0xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ee62ec-5134-466a-9753-d25ae7a80b97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# vectorize text with TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1500, stop_words=\"english\")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"source_text\"])\n",
        "X_test = vectorizer.transform(test_df[\"source_text\"])\n",
        "y_train = train_df[\"is_sensitive\"]\n",
        "y_test = test_df[\"is_sensitive\"]\n",
        "\n",
        "\n",
        "# best parameters overall according to gridsearch, but context here matters\n",
        "# with sensitive data, it is paramount that we have near 100% recall\n",
        "# this random forest below has a recall < .9\n",
        "rf = RandomForestClassifier(n_estimators = 404, max_depth = 48, min_samples_split = 3, min_samples_leaf = 3, max_features = 'sqrt', n_jobs = -1, random_state = 42, class_weight = 'balanced')\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "\n",
        "print(\"\\nTop 20 Important Words for Sensitive Detection:\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMQer7oeb67Q",
        "outputId": "b7bef9ff-d80d-4e63-ae4e-54f7640eab72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "[[ 805  103]\n",
            " [ 956 6082]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.457     0.887     0.603       908\n",
            "           1      0.983     0.864     0.920      7038\n",
            "\n",
            "    accuracy                          0.867      7946\n",
            "   macro avg      0.720     0.875     0.762      7946\n",
            "weighted avg      0.923     0.867     0.884      7946\n",
            "\n",
            "\n",
            "Top 20 Important Words for Sensitive Detection:\n",
            "com: 0.0512\n",
            "number: 0.0332\n",
            "passport: 0.0292\n",
            "username: 0.0243\n",
            "id: 0.0227\n",
            "address: 0.0193\n",
            "road: 0.0187\n",
            "ip: 0.0187\n",
            "11: 0.0186\n",
            "date: 0.0180\n",
            "00: 0.0177\n",
            "eng: 0.0176\n",
            "pm: 0.0156\n",
            "license: 0.0146\n",
            "password: 0.0130\n",
            "street: 0.0124\n",
            "time: 0.0123\n",
            "sex: 0.0122\n",
            "12: 0.0120\n",
            "email: 0.0117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install optuna"
      ],
      "metadata": {
        "id": "yYCNRCLRd58S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes like 1.5 hours to run\n",
        "# Best Params: {'n_estimators': 404, 'max_depth': 48, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}\n",
        "# Best F1 Score: 0.918767248955558\n",
        "\n",
        "\n",
        "# import optuna\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# def objective(trial):\n",
        "#     params = {\n",
        "#         'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "#         'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
        "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
        "#         'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "#         'class_weight': 'balanced',\n",
        "#         'random_state': 42\n",
        "#     }\n",
        "#     rf = RandomForestClassifier(**params)\n",
        "#     f1 = cross_val_score(rf, X_train, y_train, scoring='f1', cv=3, n_jobs=-1).mean()\n",
        "#     return f1\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=30)\n",
        "\n",
        "# print(\"Best Params:\", study.best_params)\n",
        "# print(\"Best F1 Score:\", study.best_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "Egt9TwU7duhZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings and PCA with 80% variance\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# building feature matrices from embedding column\n",
        "X_train = np.vstack(train_df[\"embedding\"].values)  # shape - (n_train, embedding_dim)\n",
        "X_test  = np.vstack(test_df[\"embedding\"].values)   # shape - (n_test, embedding_dim)\n",
        "\n",
        "y_train = train_df[\"is_sensitive\"].values\n",
        "y_test  = test_df[\"is_sensitive\"].values\n",
        "\n",
        "\n",
        "# scaling embeddings before logistic regression\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# fit pca with 114 components\n",
        "pca = PCA(n_components = 114) # 114 components to capture 80% of variance\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators = 404, max_depth = 48, min_samples_split = 3, min_samples_leaf = 3, max_features = 'sqrt', n_jobs = -1, random_state = 42, class_weight = 'balanced')\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "\n",
        "print(\"\\nTop 20 Important Words for Sensitive Detection:\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph0RtLj_LRwm",
        "outputId": "908e4d3b-a5a1-4a5f-9735-9760c57be7f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "[[ 123  785]\n",
            " [  23 7015]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.842     0.135     0.233       908\n",
            "           1      0.899     0.997     0.946      7038\n",
            "\n",
            "    accuracy                          0.898      7946\n",
            "   macro avg      0.871     0.566     0.589      7946\n",
            "weighted avg      0.893     0.898     0.864      7946\n",
            "\n",
            "\n",
            "Top 20 Important Words for Sensitive Detection:\n",
            "00: 0.1495\n",
            "000: 0.1242\n",
            "05: 0.0291\n",
            "01: 0.0244\n",
            "110: 0.0236\n",
            "103: 0.0177\n",
            "08: 0.0176\n",
            "001: 0.0145\n",
            "107: 0.0125\n",
            "157: 0.0120\n",
            "123: 0.0120\n",
            "10: 0.0108\n",
            "12: 0.0102\n",
            "15th: 0.0099\n",
            "09: 0.0095\n",
            "04: 0.0089\n",
            "117: 0.0088\n",
            "112: 0.0082\n",
            "155: 0.0077\n",
            "126: 0.0075\n"
          ]
        }
      ]
    }
  ]
}