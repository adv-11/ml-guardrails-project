{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgC6dvHlL3Fn",
        "outputId": "e481a4d7-49b5-4552-b577-673f1882252f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR0KeaqRN2hH",
        "outputId": "a4b9086c-74cb-4da9-9d57-ba23f7ddccab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmuN3rRT_gCe",
        "outputId": "2a8b947f-41b1-4fff-c7d4-37453082fd18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U0MRMIZJLqVY"
      },
      "outputs": [],
      "source": [
        "\"\"\"Python script equivalent of evaluate_model.ipynb\n",
        "\n",
        "This Colab-version allows for hyperparameters to be passed for more automation.\n",
        "(v2 refactors code to reuse the TF-IDF matrix and transformer embeddings once they have been generated if RANDOM=False)\n",
        "\n",
        "An output file will be written into a folder named \"output\".\n",
        "\n",
        "Example output: output/logreg_tf_0_C1_tol0.0001_mi500.txt\n",
        "- interpretation: Logistic Regression on a TF-IDF matrix, using class weight balancing and the following parameters: C=1, tol=0.0001, and max_iter=500\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.metrics import classification_report, roc_auc_score#, roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "#import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "SCRIPT_DIR = Path(\"/content/drive/MyDrive/guardrail/\")\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# parameters:\n",
        "RANDOM = False\n",
        "IMBALANCE = 0  # 0=class_weight, 1=undersampling\n",
        "FEATURE = 0  # 0=TF-IDF, 1=embedding, 2=both\n",
        "MODEL = 0  # 0=LogReg, 1=XGB, 2=RF\n",
        "\n",
        "# store features to reuse\n",
        "TRAIN_TF_IDF = None\n",
        "TEST_TF_IDF = None\n",
        "TRAIN_EMBEDDINGS = None\n",
        "TEST_EMBEDDINGS = None\n",
        "\n",
        "# 2nd version of feature variables if data is being undersampled\n",
        "TRAIN_TF_IDF2 = None\n",
        "TEST_TF_IDF2 = None\n",
        "TRAIN_EMBEDDINGS2 = None\n",
        "TEST_EMBEDDINGS2 = None\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Returns train/test dataframes:\n",
        "    - train_df,\n",
        "    - test_df,\n",
        "    - test_labels_df\n",
        "    \"\"\"\n",
        "    data_path = \"data\"\n",
        "    train_filename = \"train.csv\"\n",
        "    test_filename = \"test.csv\"\n",
        "    test_labels_filename = \"test_labels.csv\"\n",
        "\n",
        "    train_df = pd.read_csv(SCRIPT_DIR / data_path / train_filename)\n",
        "    test_df = pd.read_csv(SCRIPT_DIR / data_path / test_filename)\n",
        "    test_labels_df = pd.read_csv(SCRIPT_DIR / data_path / test_labels_filename)\n",
        "\n",
        "    # ignore test values with -1 labels\n",
        "    test_df_usable = test_df.loc[test_labels_df['toxic'] != -1]\n",
        "    test_labels_df_usable = test_labels_df.loc[test_labels_df['toxic'] != -1]\n",
        "\n",
        "    return train_df, test_df_usable, test_labels_df_usable\n",
        "\n",
        "\n",
        "def undersample(combined_X, combined_y):\n",
        "    \"\"\"Undersample comments with no positive labels\"\"\"\n",
        "    neg_indices = combined_y[(combined_y == 0).all(axis=1)].index.tolist()\n",
        "    pos_indices = combined_y[(combined_y != 0).any(axis=1)].index.tolist()\n",
        "\n",
        "    if RANDOM == False:\n",
        "        np.random.seed(SEED)\n",
        "\n",
        "    neg_indices = np.random.choice(neg_indices, len(pos_indices), replace=False)\n",
        "\n",
        "    test_size = 0.2\n",
        "    split_point = int(test_size * len(neg_indices))\n",
        "    np.random.shuffle(neg_indices)\n",
        "    np.random.shuffle(pos_indices)\n",
        "    train_indices = np.concat([neg_indices[split_point:], pos_indices[split_point:]])\n",
        "    test_indices = np.concat([neg_indices[:split_point], pos_indices[:split_point]])\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(test_indices)\n",
        "\n",
        "    # establish train and test datasets\n",
        "    train_X = combined_X.iloc[train_indices]\n",
        "    test_X = combined_X.iloc[test_indices]\n",
        "    train_y = combined_y.iloc[train_indices]\n",
        "    test_y = combined_y.iloc[test_indices]\n",
        "    return train_X, test_X, train_y, test_y\n",
        "\n",
        "\n",
        "def get_features(train_X, test_X):\n",
        "    global TRAIN_TF_IDF, TEST_TF_IDF, TRAIN_EMBEDDINGS, TEST_EMBEDDINGS, TRAIN_TF_IDF2, TEST_TF_IDF2, TRAIN_EMBEDDINGS2, TEST_EMBEDDINGS2\n",
        "    train_X_tf = test_X_tf = train_X_embed = test_X_embed = None\n",
        "    # check if previously generated features can be reused\n",
        "    if RANDOM == False:\n",
        "        if IMBALANCE == 1:\n",
        "            if (FEATURE == 0 or FEATURE == 2) and TRAIN_TF_IDF2 is not None:\n",
        "                train_X_tf = TRAIN_TF_IDF2\n",
        "                test_X_tf = TEST_TF_IDF2\n",
        "            if (FEATURE == 1 or FEATURE == 2) and TRAIN_EMBEDDINGS2 is not None:\n",
        "                train_X_embed = TRAIN_EMBEDDINGS2\n",
        "                test_X_embed = TEST_EMBEDDINGS2\n",
        "        else:\n",
        "            if (FEATURE == 0 or FEATURE == 2) and TRAIN_TF_IDF is not None:\n",
        "                train_X_tf = TRAIN_TF_IDF\n",
        "                test_X_tf = TEST_TF_IDF\n",
        "            if (FEATURE == 1 or FEATURE == 2) and TRAIN_EMBEDDINGS is not None:\n",
        "                train_X_embed = TRAIN_EMBEDDINGS\n",
        "                test_X_embed = TEST_EMBEDDINGS\n",
        "    # get features as normal if variables aren't set\n",
        "    if (FEATURE == 0 or FEATURE == 2) and train_X_tf is None:\n",
        "        tf_idf = TfidfVectorizer(encoding=\"utf-8\", analyzer=\"word\", ngram_range=(1,1), lowercase=True, stop_words=\"english\")\n",
        "        train_X_tf = tf_idf.fit_transform(train_X)\n",
        "        test_X_tf = tf_idf.transform(test_X)\n",
        "        if RANDOM == False:\n",
        "            if IMBALANCE == 1 and TRAIN_TF_IDF2 is None:\n",
        "                TRAIN_TF_IDF2 = train_X_tf\n",
        "                TEST_TF_IDF2 = test_X_tf\n",
        "            elif IMBALANCE == 0 and TRAIN_TF_IDF is None:\n",
        "                TRAIN_TF_IDF = train_X_tf\n",
        "                TEST_TF_IDF = test_X_tf\n",
        "    if (FEATURE == 1 or FEATURE == 2) and train_X_embed is None:\n",
        "        embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        train_X_embed = embed_model.encode(train_X.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "        test_X_embed = embed_model.encode(test_X.tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
        "        scaler = StandardScaler()\n",
        "        train_X_embed = scaler.fit_transform(train_X_embed)\n",
        "        test_X_embed = scaler.transform(test_X_embed)\n",
        "        if RANDOM == False:\n",
        "            if IMBALANCE == 1 and TRAIN_EMBEDDINGS2 is None:\n",
        "                TRAIN_EMBEDDINGS2 = train_X_embed\n",
        "                TEST_EMBEDDINGS2 = test_X_embed\n",
        "            elif IMBALANCE == 0 and TRAIN_EMBEDDINGS is None:\n",
        "                TRAIN_EMBEDDINGS = train_X_embed\n",
        "                TEST_EMBEDDINGS = test_X_embed\n",
        "    # return the queried features\n",
        "    if FEATURE == 0:\n",
        "        return train_X_tf, test_X_tf\n",
        "    elif FEATURE == 1:\n",
        "        return train_X_embed, test_X_embed\n",
        "    else:\n",
        "        train_X_both = hstack([train_X_tf, csr_matrix(train_X_embed)])\n",
        "        test_X_both = hstack([test_X_tf, csr_matrix(test_X_embed)])\n",
        "        return train_X_both, test_X_both\n",
        "\n",
        "\n",
        "def select_model(order, params=None):\n",
        "    \"\"\"initialize ML model\"\"\"\n",
        "    base = None\n",
        "    if MODEL == 1:\n",
        "        if params is None:\n",
        "            params = [100, 6, 0.3, 1]\n",
        "        params = dict(zip([\"n_estimators\", \"max_depth\", \"learning_rate\", \"min_child_weight\"], params))\n",
        "        base = XGBClassifier(**params,\n",
        "                             gamma=0,\n",
        "                             scale_pos_weight=(1 if IMBALANCE == 1 else 10),\n",
        "                             random_state=(None if RANDOM else SEED))\n",
        "    elif MODEL == 2:\n",
        "        if params is None:\n",
        "            params = [100, None, 2, 1]\n",
        "        params = dict(zip([\"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\"], params))\n",
        "        base = RandomForestClassifier(**params,\n",
        "                                      max_features=\"sqrt\",\n",
        "                                      bootstrap=True,\n",
        "                                      class_weight=(None if IMBALANCE == 1 else \"balanced\"),\n",
        "                                      random_state=(None if RANDOM else SEED))\n",
        "    else:\n",
        "        if params is None:\n",
        "            params = [1, 0.0001, 500]\n",
        "        params = dict(zip([\"C\", \"tol\", \"max_iter\"], params))\n",
        "        base = LogisticRegression(**params,\n",
        "                                  solver=\"lbfgs\",\n",
        "                                  class_weight=(None if IMBALANCE == 1 else \"balanced\"),\n",
        "                                  random_state=(None if RANDOM else SEED))\n",
        "\n",
        "    chain = ClassifierChain(estimator=base, order=order, chain_method=\"predict_proba\", cv=None,\n",
        "                            random_state=(None if RANDOM else SEED))\n",
        "    return chain\n",
        "\n",
        "\n",
        "def get_output_filename(model_params):\n",
        "    \"\"\"Return a unique filename based on model parameters\"\"\"\n",
        "    def get_param(param):\n",
        "        return model_params[f\"estimator__{param}\"]\n",
        "    models = [\"logreg\", \"xgb\", \"rf\"]\n",
        "    features = [\"tf\", \"emb\", \"both\"]\n",
        "    filename = f\"{models[MODEL]}_{features[FEATURE]}_{IMBALANCE}\"\n",
        "    if MODEL == 1:\n",
        "        filename += f\"_est{get_param('n_estimators')}_md{get_param('max_depth')}\"\n",
        "        filename += f\"_lr{get_param('learning_rate')}_g{get_param('gamma')}\"\n",
        "    elif MODEL == 2:\n",
        "        filename += f\"_est{get_param('n_estimators')}_md{get_param('max_depth')}\"\n",
        "        filename += f\"_mss{get_param('min_samples_split')}_msl{get_param('min_samples_leaf')}\"\n",
        "    else:\n",
        "        filename += f\"_C{get_param('C')}_tol{get_param('tol')}_mi{get_param('max_iter')}\"\n",
        "    filename += \".txt\"\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main(input=None):\n",
        "    # parses input list if given\n",
        "    params = None\n",
        "    if input is not None:\n",
        "        global RANDOM, IMBALANCE, FEATURE, MODEL\n",
        "        RANDOM = input[0]\n",
        "        IMBALANCE = input[1]\n",
        "        FEATURE = input[2]\n",
        "        MODEL = input[3]\n",
        "        params = input[4:]\n",
        "\n",
        "    # load dataset\n",
        "    print(\"Loading data...\")\n",
        "    train_df, test_df, test_labels_df = load_data()\n",
        "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # combine pre-selected train/test data\n",
        "    combined_X = pd.concat([train_df['comment_text'], test_df['comment_text']])\n",
        "    combined_y = pd.concat([train_df[label_cols], test_labels_df[label_cols]])\n",
        "\n",
        "    # establish train and test datasets\n",
        "    train_X = test_X = train_y = test_y = None\n",
        "    if IMBALANCE == 1:\n",
        "        train_X, test_X, train_y, test_y = undersample(combined_X, combined_y)\n",
        "    else:\n",
        "        if RANDOM == False:\n",
        "            train_X, test_X, train_y, test_y = train_test_split(combined_X, combined_y, train_size=0.8, test_size=0.2, random_state=SEED)\n",
        "        else:\n",
        "            train_X, test_X, train_y, test_y = train_test_split(combined_X, combined_y, train_size=0.8, test_size=0.2, random_state=SEED)\n",
        "    #print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)\n",
        "    msg_data = f\"Train/Test datasets: {train_X.shape}, {test_X.shape}, {train_y.shape}, {test_y.shape}\\n\"\n",
        "\n",
        "    # get features\n",
        "    print(\"Getting features...\")\n",
        "    train_X, test_X = get_features(train_X, test_X)\n",
        "    msg_features = f\"\\nShape of features: {train_X.shape}, {test_X.shape}\\n\"\n",
        "\n",
        "    # initialize and train model\n",
        "    order = [4, 2, 0, 1, 5, 3]\n",
        "    model = select_model(order, params)\n",
        "\n",
        "    model_params = model.get_params(deep=True)\n",
        "    msg_params = \"\\nModel Parameters\\n\" + \"-\" * 20 + \"\\n\"  #print(\"Model Parameters\", \"-\" * 20, sep='\\n')\n",
        "    for key, val in model_params.items():\n",
        "        msg_params += f\"{key}: {val}\\n\"  #print(f\"{key}: {val}\")\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    start = time.perf_counter()\n",
        "    model.fit(train_X, train_y)\n",
        "    end = time.perf_counter()\n",
        "    msg_time = f\"\\nTraining Time: {(end-start)//60}m {(end-start)%60:.3f}s\\n\"\n",
        "\n",
        "    # test and evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    pred = model.predict(test_X)\n",
        "    pred_proba = model.predict_proba(test_X)\n",
        "\n",
        "    msg_accuracy = f\"\\nAccuracy: {round(model.score(test_X, test_y), 4)}\\n\"\n",
        "    #print(f\"\\nAccuracy: {round(model.score(test_X_tf, test_y), 4)}\")\n",
        "    msg_report = classification_report(y_true=test_y, y_pred=pred, target_names=[label_cols[idx] for idx in order], zero_division=np.nan)\n",
        "    #print(classification_report(y_true=test_y, y_pred=pred, target_names=[label_cols[idx] for idx in order], zero_division=np.nan))\n",
        "\n",
        "    y_true_bin = label_binarize(test_y, classes=[label_cols[idx] for idx in order])\n",
        "\n",
        "    # Compute ROC curve and AUC for each class\n",
        "    msg_auc = \"\\nAUC:\\n\"  #print(\"\\nAUC:\")\n",
        "    for i in range(y_true_bin.shape[1]):\n",
        "        roc_auc = roc_auc_score(y_true_bin[:, i], pred_proba[:, i])\n",
        "        msg_auc += f\"Class {i}: {label_cols[order[i]].ljust(15)} (score = {roc_auc:.2f})\\n\"\n",
        "        #print(f\"Class {i}: {label_cols[order[i]].ljust(15)} (score = {roc_auc:.2f})\")\n",
        "\n",
        "    # output msg strings\n",
        "    msg = msg_data + msg_features + msg_params + msg_time + msg_accuracy + msg_report + msg_auc\n",
        "    output_path = SCRIPT_DIR / \"output\" / get_output_filename(model_params)\n",
        "    output_path.parent.mkdir(exist_ok=True)\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(msg)\n",
        "    print(f\"Script output written to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input guidelines:\n",
        "# - first four set the parameters: RANDOM, IMBALANCE, FEATURE, and MODEL\n",
        "# - the next parameters set the model hyperparameters (e.g. the defaults for each model)\n",
        "#   - LogReg: C, tol, max_iter\n",
        "#     - e.g. [False, 0, 0, 0, 1, 0.0001, 500]\n",
        "#   - XGB: n_estimators, max_depth, learning_rate, min_child_weight\n",
        "#     - e.g. [False, 0, 0, 1, 100, 6, 0.3, 1]\n",
        "#   - RF: n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
        "#     - e.g. [False, 0, 0, 2, 100, 6, 2, 1]\n",
        "#     - (technically the default max_depth is None, but that would take forever)\n",
        "\n",
        "for md in [2, 4, 8, 10]:\n",
        "    input = [False, 1, 2, 1, 100, md, 0.5, 1]\n",
        "    main(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK4jHeO5OtZz",
        "outputId": "576b25c7-4af0-4e0a-d683-9cd2f30f413b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Getting features...\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Script output written to /content/drive/MyDrive/guardrail/output/xgb_both_1_est100_md2_lr0.5_g0.txt\n",
            "Loading data...\n",
            "Getting features...\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Script output written to /content/drive/MyDrive/guardrail/output/xgb_both_1_est100_md4_lr0.5_g0.txt\n",
            "Loading data...\n",
            "Getting features...\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Script output written to /content/drive/MyDrive/guardrail/output/xgb_both_1_est100_md8_lr0.5_g0.txt\n",
            "Loading data...\n",
            "Getting features...\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Script output written to /content/drive/MyDrive/guardrail/output/xgb_both_1_est100_md10_lr0.5_g0.txt\n"
          ]
        }
      ]
    }
  ]
}