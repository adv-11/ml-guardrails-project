# ml-guardrails-project
semester project for 257-ml

![poster](poster_image.png)

# contributors

1. adv ( Advait Shinde )
2. jasss ( Jasper Morgal )
3. toneyyyyy ( Toney Zhen )
4. nickk ( Nickzad Bayati )

# about the project

Guardrails:
- Input: Sensitive Data (Toney Zhen)
- Input: Prompt Injection (Advait Shinde)
- Output: General Malicious Output (Jasper Morgal)
- Output: Toxicity (Nickzad Bayati)

# instructions

Output: Toxicity
1. download dataset from Kaggle: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)
2. unzip files and move them into `/output_guardrails/toxicity/data/`
3. use `evaluate_model2.ipynb` to train/evaluate models
